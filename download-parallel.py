import feedparser
import requests
import os
from bs4 import BeautifulSoup
import requests
from urlparse import urlparse
from multiprocessing import Pool


def parse_feed(url):
    _links = []
    p = feedparser.parse(url)
    print p.feed.title
    print p.feed.link
    print p.feed.description

    for entry in p.entries:
        _links.append(entry.link)
    return _links


def get_local_files(path):
    _path = path
    extensions = ['pcap']
    files = [fn for fn in os.listdir(_path) if any([fn.endswith(ext) for ext in extensions])]
    return files


def get_file_links(link):
    pcaps = []
    req = requests.get(link)
    data = req.text
    bs = BeautifulSoup(data)
    for l in bs.find_all('a'):
        if 'pcap' in str(l.get('href')):
            pcap = str(l.get('href'))
            u = urlparse(link)
            patharray = str(u.path).split('/')
            file_url = 'http://' + u.netloc + '/'.join(patharray[:-1]) + '/' + pcap
            pcaps.append((pcap, file_url))
    return pcaps


def download_files(file):
    _file = file
    print "Downloading %s\n" % _file
    r = requests.get(_file)
    local_filename = path + _file.split('/')[-1]
    with open(local_filename, 'wb') as f:
        for chunk in r.iter_content(chunk_size=1024):
            if chunk:
                f.write(chunk)
    return "Complete"

url = 'http://www.malware-traffic-analysis.net/blog-entries.rss'
path = './pcaps/'

local_files = get_local_files(path)
links = parse_feed(url)
print "%s pages to scan" % len(links)

pool = Pool(processes=20)
results = pool.map(get_file_links, links)
files_to_download = []

for result in results:
    for r in result:
        if type(r) is tuple:
            pcap, url = r
            if pcap not in local_files:
                files_to_download.append(url)

download_results = pool.map(download_files, files_to_download)

print "Downloaded {} files".format(len(download_results))


